{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The Most Educated Places in the U.S."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier this week I came across an article on the [most educated places in the US](\n",
    "http://www.businessinsider.com/most-educated-places-map-2014-9). I thought it would be cool to try to recreate their findings and expand upon by taking into account the uncertainty of the survey results.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data\n",
    "\n",
    "We need two variables for each place in the US:\n",
    "* Percent of population over 25 with a bachelor's degree or higher (called educational attainment from now on)\n",
    "* Total population\n",
    "\n",
    "We'll also grab the margin of error for these variables. We'll use them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def state_and_place(df):\n",
    "    \"\"\"Extracts state and place from the original identification variable.\"\"\"\n",
    "    columns = (\n",
    "        df\n",
    "        .id\n",
    "        .astype(str)\n",
    "        .str.extract('(?P<state_fips>\\d{1,2})(?P<place_fips>\\d{5})', expand=True)\n",
    "        .astype(int)\n",
    "    )\n",
    "    return df.join(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_acs(year):\n",
    "    \"\"\"Reads and formats ACS educational attainment data for a given year.\"\"\"\n",
    "    filename = '../data/ACS_{year}_5YR_S1501/ACS_{year}_5YR_S1501_with_ann.csv'.format(year=str(year)[-2:])\n",
    "    \n",
    "    na_values = ('**', '-', '+', '***', '*****', 'N', '(X)')\n",
    "\n",
    "    variables = {\n",
    "        'Id2': 'id',\n",
    "        'Geography': 'place',\n",
    "        \"Total; Estimate; Percent bachelor's degree or higher\": 'pct_bachelor_plus',\n",
    "        \"Total; Margin of Error; Percent bachelor's degree or higher\": 'pct_bachelor_plus_moe'\n",
    "    }\n",
    "    \n",
    "    return (\n",
    "        pd\n",
    "        .read_csv(\n",
    "            filename,\n",
    "            usecols=variables.keys(),\n",
    "            na_values=na_values,\n",
    "            # https://www.census.gov/geo/maps-data/data/tiger/char_encoding.html\n",
    "            encoding='ISO-8859-1',\n",
    "            skiprows=1\n",
    "        )\n",
    "        .rename(columns=variables)\n",
    "        .pipe(state_and_place)\n",
    "        .drop('id', axis=1)\n",
    "        .set_index(['state_fips', 'place_fips'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_population(year):\n",
    "    \"\"\"Collects and formats population data from the Census API for a given year.\"\"\"\n",
    "    url = 'http://api.census.gov/data/{}/acs5?get=B01003_001E,B01003_001M&for=place:*'.format(year)\n",
    "    r = requests.get(url)\n",
    "    data = r.json()\n",
    "    \n",
    "    columns = {\n",
    "        'B01003_001E': 'population',\n",
    "        'B01003_001M': 'population_moe',\n",
    "        'state': 'state_fips',\n",
    "        'place': 'place_fips'\n",
    "    }\n",
    "    \n",
    "    return (\n",
    "        pd.DataFrame(data=data[1:], columns=data[0])\n",
    "        .rename(columns=columns)\n",
    "        .apply(lambda s: s.astype(int))\n",
    "        .set_index(['state_fips', 'place_fips'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def state(df):\n",
    "    \"\"\"Converts state FIPS codes to state names.\"\"\"\n",
    "    data = (\n",
    "        df\n",
    "        .index\n",
    "        .get_level_values('state_fips')\n",
    "        .astype(str)\n",
    "        .str.rjust(2, fillchar='0')\n",
    "    )\n",
    "    s = pd.Series(data=data, index=df.index)\n",
    "    \n",
    "    return s.apply(lambda x: us.states.lookup(x).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(year):\n",
    "    \"\"\"Reads and formats desired ACS data for a given year.\"\"\"\n",
    "    acs = read_acs(year)\n",
    "    population = read_population(year)\n",
    "    \n",
    "    return (\n",
    "        acs\n",
    "        .join(population)\n",
    "        .assign(state=state)\n",
    "        .set_index(['state', 'place'])\n",
    "        .sort_index(axis=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = read_data(2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check out the dataset\n",
    "\n",
    "Whenever I work with a dataset for the first time I like to do a few things:\n",
    "* Check out the shape/composition.\n",
    "* Look at the top few rows.\n",
    "* Look at the bottom two rows.\n",
    "* Get a sense of how things are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Composition\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Top\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bottom\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Summary\n",
    "df.dropna().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population vs. Attainment\n",
    "\n",
    "We can see right away that we're dealing with some pretty heavily skewed data. Most places are teeny-tiny, while some are massive (I see you New York). Most also tend to have educational attainment rates (i.e., percent of the population over 25 with bachelor's degrees) around 10-20%, though some go all the way to 100%! More on that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.jointplot('population', 'pct_bachelor_plus', data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreating the original metric\n",
    "\n",
    "To make sure I'm starting off right, I wanted to try to recreate the original ranking. Fortunately the author was explicit with his methodology, so recreating was a breeze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colleges = (\n",
    "    'Stanford CDP, California',\n",
    "    'University at Buffalo CDP, New York',\n",
    "    'University of Pittsburgh Johnstown CDP, Pennsylvania',\n",
    "    'Mississippi State CDP, Mississippi',\n",
    "    'St. Vincent College CDP, Pennsylvania',\n",
    "    'University CDP, Mississippi'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highest_attainment_orig(df, filter_=True):\n",
    "    \"\"\"Gets the place with the higehst educational attainment in each state.\"\"\"\n",
    "    if filter_:\n",
    "        # Only include places with at least 1,000 people, excluding colleges\n",
    "        subset = df.loc[(df.population > 1000) & (df.index.map(lambda x: x[1] not in colleges))]\n",
    "    else:\n",
    "        subset = df\n",
    "    # Find the place with the higehst attainment by state\n",
    "    idx = (\n",
    "        subset\n",
    "        .groupby(level='state')\n",
    "        .pct_bachelor_plus\n",
    "        .transform('max')\n",
    "        .eq(subset.pct_bachelor_plus)\n",
    "    )\n",
    "    \n",
    "    return subset.loc[idx].reset_index('state', drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .pipe(highest_attainment_orig)\n",
    "    .pct_bachelor_plus\n",
    "    .sort_values(ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the original list, for comparison:\n",
    "![top_places](http://static4.businessinsider.com/image/5410991d6bb3f70a0c123b31-960/most%20educated%20places%20table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The Problem: Uncertainty\n",
    "\n",
    "One thing I was curious about was why the author decided to include only places with more than 1,000 people. What does this list look like if we include all places?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .pipe(highest_attainment_orig, filter_=False)\n",
    "    .sort_values('pct_bachelor_plus', ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of 100%s. One thing to notice is that the confidence interval for these places are massive. Zion CDP, Oklahoma, for example, has a 90% CI of [0, 100]. The interval includes every value this variable can take! While filtering these small, highly-uncertain places is a good heuristic, there are more robust ways to handle this isuse.\n",
    "\n",
    "One way is to use the 90% least plausible value, defined as the value such that there is only a 10% chance the true parameter is lower. In other words, the smartest places, according to this procedure, are the places that are _most likely_ to have a high percentage of people with educational attainment. This works well because even if we completely overestimate the educational attainment rates we can be sure the top places are still on top.\n",
    "\n",
    "The great thing about the [American Community Survey (ACS)](https://www.census.gov/programs-surveys/acs/about.html) is that they provide margins of error (MoE) for all of their estimate variables. Best of all, their MoE are for 90% confidence intervals. All we need to do is subtract the MoE from the estimate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A more conservative approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lower_bound(df, column):\n",
    "    \"\"\"Gets the 90% confidence interval lower bound for a variable.\"\"\"\n",
    "    lower = df[column] - df['{}_moe'.format(column)]\n",
    "    # Minimum value is 0\n",
    "    lower.loc[lower < 0] = 0\n",
    "    \n",
    "    return lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_lower_bounds(df):\n",
    "    \"\"\"Adds the lower bounds for population and educational attainment.\"\"\"\n",
    "    return (\n",
    "        df\n",
    "        .assign(\n",
    "            population_lower=lambda df: df.pipe(lower_bound, 'population'),\n",
    "            pct_bachelor_plus_lower=lambda df: df.pipe(lower_bound, 'pct_bachelor_plus')\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.pipe(add_lower_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.jointplot('population', 'pct_bachelor_plus_lower', data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the educational attainment distribution is even more skewed than before! This is because so many places are small and therefore have a lot of uncertainty associated with their estimates. These places will be unlikely to show up on our list (as they shouldn't)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highest_attainment(df):\n",
    "    \"\"\"Find the place with the higehst attainment by state, using the lower bound.\"\"\"\n",
    "    subset = df.loc[df.index.map(lambda x: x[1] not in colleges)]\n",
    "    idx = (\n",
    "        subset\n",
    "        .groupby(level='state')\n",
    "        .pct_bachelor_plus_lower\n",
    "        .transform('max')\n",
    "        .eq(subset.pct_bachelor_plus_lower)\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        subset\n",
    "        .loc[idx]\n",
    "        .reset_index('state', drop=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_highest(df):\n",
    "    \"\"\"Creates an errorbar plot for attainment data.\"\"\"\n",
    "    highest = df.sort_values('pct_bachelor_plus_lower')\n",
    "\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    plt.errorbar(\n",
    "        highest.pct_bachelor_plus,\n",
    "        np.arange(len(highest)),\n",
    "        xerr=highest.pct_bachelor_plus_moe,\n",
    "        fmt='o'\n",
    "    )\n",
    "\n",
    "    plt.xlim(40, 100)\n",
    "    plt.ylim(-1, len(highest))\n",
    "    plt.yticks(np.arange(len(highest)), highest.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.pipe(highest_attainment).pipe(plot_highest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "In the plot above we have the point estimate (the dot) and its confidence interval, ordered by the 90% least plausible value (i.e., the lower end of the CI). Compared to the original results, about half of the top places have changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do things look now?\n",
    "\n",
    "The original data is a few years old. Let's see how things look with 2010-2014 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_14 = read_data(2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_14.pipe(add_lower_bounds).pipe(highest_attainment).pipe(plot_highest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "\n",
    "I don't think places are a good unit of analysis for this kind of work. Many are subsets of larger cities, which can be misleading.\n",
    "\n",
    "An alternative would be to use metropolitian/micropolitan statistical areas and see how things change. The issue there is than smaller geographies are underrepresented (or not represented at all).\n",
    "\n",
    "I'd like to try a hybrid approach: Use MSA where possible, otherwise use place. That allows us to acknowledge agglomeration effects, where applicable. I would need to find a mapping of places to MSAs. If that doesn't exist (quick Google search suggest it doesn't), I'd have to create one using Census TIGER shapefiles, which is easy to do with something like PostGIS or QGIS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements\n",
    "\n",
    "Huge thanks goes to [Andy Kiersz](https://twitter.com/AndyKiersz), the author of the original article, for the inspiration.\n",
    "\n",
    "Much of the discussion on uncertainty comes from [Cameron Davidson-Pilon's](https://twitter.com/Cmrn_DP) book [Bayesian Methods for Hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers). Can't recommend it enough."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
